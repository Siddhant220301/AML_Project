{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U29QupDatqbd"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from pickle import load\n",
        "from numpy import array\n",
        "from pickle import dump\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.models import load_model\n",
        "from keras.utils import to_categorical\n",
        "from keras_preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text"
      ],
      "metadata": {
        "id": "8s_xEJBfuL1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save tokens to file, one dialog per line\n",
        "def save_doc(lines, filename):\n",
        "\tdata = '\\n'.join(lines)\n",
        "\tfile = open(filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()"
      ],
      "metadata": {
        "id": "_F9bzaTkuQGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load text\n",
        "input_path = \"D:\\\\AML-Project\\\\Raw data\\\\\"\n",
        "output_path = \"D:\\AML-Project\\Data\\\\\"\n",
        "in_filename = 'elonmusk_tweets.csv'"
      ],
      "metadata": {
        "id": "mqhhRxL6uTY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "with open(input_path + in_filename) as csv_file:\n",
        "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "    line_count = 0\n",
        "    for row in csv_reader:\n",
        "        if line_count == 0:\n",
        "            line_count+=1\n",
        "        else:\n",
        "            low = row[2][2:-1].lower()\n",
        "            sentences.append(low)\n",
        "            line_count+=1\n",
        "    print(f'Processed {line_count} lines.')\n",
        "    \n",
        "raw_text = \" \".join(sentences)\n",
        "print(sentences)"
      ],
      "metadata": {
        "id": "taqCi73PuWWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean\n",
        "count = 0\n",
        "for tweet in sentences:\n",
        "    temp = tweet.split()\n",
        "    for word in temp:\n",
        "        if '/' in word:\n",
        "            temp.remove(word)\n",
        "        elif '\\\\' in word:\n",
        "            temp.remove(word)\n",
        "        elif 'rt' in word:\n",
        "            temp.remove(word)\n",
        "        elif '@' in word:\n",
        "            temp.remove(word)\n",
        "    sentences[count] = temp\n",
        "    count += 1\n",
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlQQleg_ud1F",
        "outputId": "697b7c08-ada1-48e1-add3-a4b874634c24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# organize into sequences of characters\n",
        "sequences = list()\n",
        "for tweet in sentences:\n",
        "    for i in range(len(tweet)-4):\n",
        "        seq = tweet[i]+\" \"+tweet[i+1]+\" \"+tweet[i+2]+\" \"+tweet[i+3]+\" \"+tweet[i+4]\n",
        "        sequences.append(seq)\n",
        "print('Total Sequences: %d' % len(sequences))\n",
        "sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii9i-m60uhyw",
        "outputId": "8fcf171f-b6fe-4829-b78d-9ffbb3f4c13b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Sequences: 0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save sequences to file\n",
        "filename = 'elon_sequences_WL1.txt'\n",
        "out_filename = output_path+filename\n",
        "save_doc(sequences, out_filename)\n",
        "sequences"
      ],
      "metadata": {
        "id": "VBKgvJqluljo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = 'C:\\\\Users\\\\iD Student\\\\Documents\\\\PPNN\\\\Processed Data\\\\'\n",
        "# load the model\n",
        "model = load_model(input_path+'model_WL1.h5')"
      ],
      "metadata": {
        "id": "2ykfa1BOupL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the mapping\n",
        "mapping = load(open(input_path+'mapping_WL1.pkl', 'rb'))"
      ],
      "metadata": {
        "id": "AmnbTwBAwQZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate a sequence of characters with a language model\n",
        "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
        "\tin_text = seed_text\n",
        "\t# generate a fixed number of characters\n",
        "\tfor _ in range(n_chars):\n",
        "\t\t# encode the characters as integers\n",
        "\t\tencoded = [mapping[word] for word in in_text.split()]\n",
        "\t\t# truncate sequences to a fixed length\n",
        "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "\t\t# one hot encode\n",
        "\t\tencoded = to_categorical(encoded, num_classes=len(mapping))\n",
        "\t\tencoded = encoded.reshape(1, encoded.shape[1], encoded.shape[2])\n",
        "\t\t# predict character\n",
        "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
        "\t\t# reverse map integer to character\n",
        "\t\tout = ''\n",
        "\t\tfor word, index in mapping.items():\n",
        "\t\t\tif index == yhat:\n",
        "\t\t\t\tout = word\n",
        "\t\t\t\tbreak\n",
        "\t\t# append to input\n",
        "\t\tin_text = in_text+ \" \" + out\n",
        "\treturn in_text"
      ],
      "metadata": {
        "id": "X85zxY-6wSw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "print(generate_seq(model, mapping, 2, 'got mars', 20))\n",
        "# test\n",
        "print(generate_seq(model, mapping, 2, 'spend some', 20))\n",
        "# test\n",
        "print(generate_seq(model, mapping, 2, 'solar power', 20))"
      ],
      "metadata": {
        "id": "6mvGNOUxwW0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text"
      ],
      "metadata": {
        "id": "uoqoEB7xwaVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load\n",
        "input_path = \"D:\\\\Github\\\\PPPNN\\\\Processed Data\\\\\"\n",
        "in_filename = 'elon_sequences_WL1.txt'\n",
        "raw_text = load_doc(input_path+in_filename)\n",
        "lines = raw_text.split('\\n')"
      ],
      "metadata": {
        "id": "qdyJbhfPwnMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = []\n",
        "for line in lines:\n",
        "\ttemp_words = line.split()\n",
        "\tfor word in temp_words:\n",
        "\t\tif word not in words:\n",
        "\t\t\twords.append(word)\n",
        "\n",
        "mapping = dict((c, i) for i, c in enumerate(words))\n",
        "print(mapping)"
      ],
      "metadata": {
        "id": "4Pc7adjHwpxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = list()\n",
        "for line in lines:\n",
        "\t# integer encode line\n",
        "\tencoded_seq = [mapping[word] for word in line.split()]\n",
        "\t# store\n",
        "\tsequences.append(encoded_seq)\n",
        "print(sequences)"
      ],
      "metadata": {
        "id": "BJSkarGowsep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vocabulary size\n",
        "vocab_size = len(mapping)\n",
        "print('Vocabulary Size: %d' % vocab_size)"
      ],
      "metadata": {
        "id": "0xyH4ToZwvYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = array(sequences)\n",
        "print(sequences)\n",
        "X, y = sequences[:,:-3], sequences[:,-3]"
      ],
      "metadata": {
        "id": "En5XGXsDwyM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
        "X = array(sequences)\n",
        "y = to_categorical(y, num_classes=vocab_size)"
      ],
      "metadata": {
        "id": "QW8ld2U8w1TL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(LSTM(1000, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "01K6yqhlw3zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit model\n",
        "model.fit(X, y, epochs=100, verbose=2)"
      ],
      "metadata": {
        "id": "tNkEHnWxxAqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model to file\n",
        "model.save(input_path+'model_WL1.h5')\n",
        "# save the mapping\n",
        "dump(mapping, open(input_path+'mapping_WL1.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "bYl0d4vqxDmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"aaa\")"
      ],
      "metadata": {
        "id": "go0OUQR3xGF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h7JnA3y4xIc7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}